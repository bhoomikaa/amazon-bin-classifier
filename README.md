\# ğŸ§  Amazon Bin Object Classifier  



This project builds a \*\*visionâ€“language model\*\* that classifies and counts objects in warehouse bins using \*\*CLIP (Contrastive Languageâ€“Image Pretraining)\*\* and \*\*EfficientNet\*\*.  

It demonstrates how modern multimodal AI models can perform \*\*zero-shot classification\*\* â€” recognizing concepts theyâ€™ve never been explicitly trained on â€” and how fine-tuning can further improve performance.



---



\## ğŸ” Project Overview  



In large warehouse environments, sorting and identifying the number or type of items inside storage bins is a repetitive task that can benefit from automation.  

This project explores two complementary approaches:  



1\. \*\*Zero-Shot CLIP Classification\*\* â€“ Using OpenAIâ€™s CLIP model to match image embeddings with text prompts like â€œa photo of a bin containing 3 items.â€  

2\. \*\*Fine-Tuned EfficientNet\*\* â€“ Training a supervised classifier on labeled bin images to improve counting accuracy.  



The goal is to compare both methods and understand how well pretrained vision-language models generalize to object-counting problems.



---



\## âš™ï¸ Key Features  

\- ğŸ§© Image preprocessing pipeline (resize, normalize, augment).  

\- ğŸ”¡ CLIP-based embedding generation for images and text.  

\- ğŸ¤– Zero-shot classification without any training.  

\- ğŸ§  EfficientNet fine-tuning for supervised learning.  

\- ğŸ“Š Evaluation metrics: accuracy, mean absolute error (MAE), and confusion matrices.  

\- ğŸ’¾ Modular notebooks for each stage â€” preprocessing, embeddings, metrics, and model training.



---



\## ğŸ§° Tech Stack  

\- \*\*Language:\*\* Python 3.10+  

\- \*\*Libraries:\*\* PyTorch, OpenCLIP, timm, NumPy, Pandas, scikit-learn, Matplotlib, tqdm  

\- \*\*Hardware:\*\* CPU / GPU compatible  



---



\## ğŸ“ Repository Structure  

```

amazon-bin-classifier/

â”œâ”€â”€ image\_preprocessing.ipynb        # Preprocessing and augmentation

â”œâ”€â”€ cLip\_img\_embeddings.ipynb        # CLIP image embeddings

â”œâ”€â”€ Text\_Embeddings.ipynb            # CLIP text embeddings

â”œâ”€â”€ Object\_Counting\_Zero\_Shot\_CLIP.ipynb # Zero-shot experiments

â”œâ”€â”€ clip\_metrics\_compute.ipynb       # Metrics \& evaluation

â”œâ”€â”€ CLIP\_Efficient\_Net\_Train.py      # Fine-tune EfficientNet

â”œâ”€â”€ cleaned\_data.csv                 # Sample labeled data

â””â”€â”€ README.md                        # Project documentation

```



---



\## ğŸš€ How to Run  



1\. \*\*Install dependencies\*\*

&nbsp;  ```bash

&nbsp;  pip install -r requirements.txt

&nbsp;  ```



2\. \*\*Run preprocessing\*\*

&nbsp;  ```bash

&nbsp;  jupyter notebook image\_preprocessing.ipynb

&nbsp;  ```



3\. \*\*Generate embeddings\*\*

&nbsp;  ```bash

&nbsp;  jupyter notebook cLip\_img\_embeddings.ipynb

&nbsp;  ```



4\. \*\*Evaluate zero-shot classification\*\*

&nbsp;  ```bash

&nbsp;  jupyter notebook Object\_Counting\_Zero\_Shot\_CLIP.ipynb

&nbsp;  ```



5\. \*(Optional)\* Fine-tune the EfficientNet model

&nbsp;  ```bash

&nbsp;  python CLIP\_Efficient\_Net\_Train.py

&nbsp;  ```



---



\## ğŸ“ˆ Example Results  

| Model | Top-1 Accuracy | MAE | Description |

|-------|----------------|-----|--------------|

| CLIP (Zero-Shot) | 78 % | 1.3 | Uses natural-language prompts, no training |

| EfficientNet-B0 (Fine-Tuned) | 93 % | 0.5 | Trained for 10 epochs on labeled data |



---



\## ğŸ”® Future Work  

\- Add object-detection to locate items within bins.  

\- Experiment with ViT or Swin-Transformer backbones.  

\- Deploy inference as a REST API or Streamlit demo.  



---



\## ğŸ‘©â€ğŸ’» Author  

\*\*Bhoomika\*\*  

Built as a personal deep-learning project to explore vision-language models and warehouse automation.



---



\## ğŸ“œ License  

MIT License â€” free for research and educational use.  



